{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "modelname = 'src/model1.1.1_pan/'\n",
    "os.system('mkdir -p ./'+modelname)\n",
    "# 创建显示数据信息文件outputf\n",
    "outputf = open(modelname+'/output.txt', 'w')\n",
    "outputf.write('OMP_NUM_THREADS = '+str(os.popen('echo $OMP_NUM_THREADS').read()) +'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import keras, os, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total  465 cosmologies\n",
      "Build up gridfile_dict... (for speed-up of load_grid()) \n",
      "\tmissing cosmology! om0.180_As2.040\n",
      "\tmissing cosmology! om0.180_As2.100\n",
      "\tmissing cosmology! om0.180_As2.120\n",
      "\tmissing cosmology! om0.180_As2.140\n",
      "\tmissing cosmology! om0.180_As2.200\n",
      "\tmissing cosmology! om0.180_As2.220\n",
      "\tmissing cosmology! om0.180_As2.260\n"
     ]
    }
   ],
   "source": [
    "#lsstr = \"ls /home/xiaodongli/data/colas/cola_multiverse/om_As/\"\n",
    "lsstr = \"ls /media/cosmo/Seagate/cola_multiverse/om_As/\"\n",
    "\n",
    "# 本代码内无用\n",
    "def cosmostr(om, As):\n",
    "    return 'om%.3f' % om + '_As%.3f' % As\n",
    "\n",
    "# 本代码内无用\n",
    "def snpfiles(cosmology, snpstr='c'):  #\n",
    "    return os.popen(lsstr + cosmology + \"*snap*\" + snpstr + \".*\").read().split()\n",
    "\n",
    "# 打开含grid内容的文件\n",
    "def gridfiles(cosmology, snpstr='c'):\n",
    "    return os.popen(lsstr + cosmology + \"*grid*\" + snpstr + \".*\").read().split()\n",
    "\n",
    "# 返回[sigma8, r m]\n",
    "def mocklist():\n",
    "    # *代替多个字母,列出符合的.lua文件（列表类型，因数目非一个）,lua文件包含宇宙学参数信息等\n",
    "    files = os.popen(lsstr + \"om*.lua\").read().split('\\n')  # 列表类型\n",
    "    cosmologies = []  # 宇宙学样本名称列表\n",
    "    mocks = {}  # 模拟的数据参数\n",
    "    ifile = 0  # 有效文件\n",
    "    for nowfile in files:\n",
    "        nowstr = nowfile[-39:-10]\n",
    "        # eg. om0.160_As2.000_sigma8_0.4252_init\n",
    "        # str[a:b]不存在时,返回'',不存在则忽略\n",
    "        if nowstr == '':\n",
    "            continue\n",
    "        cosmologies.append(nowstr[0:15])  #添加宇宙学标签：om0.160_As2.000\n",
    "        ifile += 1\n",
    "        try:\n",
    "            mocks[nowstr[0:15]] = {'om': float(nowstr[2:7]), 'As': float(nowstr[10:15]),\n",
    "                                   'sigma8': float(nowstr[23:29])}\n",
    "            # 添加ranseed标签\n",
    "            ranseed = float(open(nowfile, 'r').readline().split()[2])\n",
    "            mocks[nowstr[0:15]]['ranseed'] = int(ranseed)\n",
    "            # print(ranseed)\n",
    "        except:\n",
    "            pass\n",
    "    return cosmologies, files, mocks  # 名称列表， 样本文件， 参数列表\n",
    "# grid样本集合\n",
    "gridfile_dict = {}\n",
    "# 提取样本数据\n",
    "cosmologies, filenames, infos = mocklist()\n",
    "print('In total ', len(cosmologies), 'cosmologies')\n",
    "outputf.write('In total '+str(len(cosmologies))+'cosmologies\\n')\n",
    "print('Build up gridfile_dict... (for speed-up of load_grid()) ')\n",
    "outputf.write('Build up gridfile_dict... (for speed-up of load_grid()) \\n')\n",
    "for cosmology in cosmologies:\n",
    "    rlt = gridfiles(cosmology)  # 打开标签相应的grid文件(列表类型）\n",
    "    if rlt == []:\n",
    "        print ('\\tmissing cosmology!', cosmology)\n",
    "        outputf.write('\\tmissing cosmology!' +str(cosmology)+ '\\n')\n",
    "    else:\n",
    "        gridfile_dict[cosmology] = rlt[0]\n",
    "np.random.shuffle(cosmologies)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid(gridfile, snpstr='c', printinfo=False):  # 网格加载\n",
    "    #gridfile = os.popen(lsstr + cosmology+\"_sigma8_*grid*\" + snpstr + \".*\").read().split()[0]\n",
    "    #print('load in gridfile : ', gridfile, '...')\n",
    "    nowf = open(gridfile, 'rb')  # 以二进制形式读取文件\n",
    "    # struct:对python基本类型值与用python字符串格式表示的C struct类型间转化\n",
    "    size = struct.unpack('f' * 1, nowf.read(4 * 1))[0]\n",
    "    grid_nc = struct.unpack('i' * 1, nowf.read(4 * 1))[0]\n",
    "    data = struct.unpack('f' * grid_nc ** 3, nowf.read(4 * grid_nc ** 3))\n",
    "    if printinfo:\n",
    "        print('read in box size     \\n\\t', size)\n",
    "        print('read in num_grid      \\n\\t', grid_nc)\n",
    "        print('read in coarse grid \\n\\tsize    : ', len(data), '\\n\\texpect  : ', grid_nc ** 3)\n",
    "\n",
    "    nowf.close()\n",
    "    return np.array(data).reshape((grid_nc, grid_nc, grid_nc))\n",
    "\n",
    "def subcubes(A):\n",
    "    rlt = []\n",
    "    for row1 in [0, 32, 64, 96]:\n",
    "        for row2 in [0, 32, 64, 96]:\n",
    "            for row3 in [0, 32, 64, 96]:\n",
    "                rlt.append(A[row1:row1+32,row2:row2+32,row3:row3+32])\n",
    "    return rlt\n",
    "\n",
    "def data_augument(A):\n",
    "    rlt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "batch_size = 6\n",
    "num_subcube = 64\n",
    "num_data_augument = 48\n",
    "\n",
    "x_test = np.zeros((int(test_size*batch_size)+1,32,32,32, 1))\n",
    "y_test = np.zeros((int(test_size*batch_size)+1, 2))\n",
    "\n",
    "\n",
    "###  xiaodong: 重新写了 load_grid 程序。。。之前有错误！！！！（好像只会 load 进来一个 om...)\n",
    "def train_generator():  # 必须无限循环yield数据,全部数据遍历后再重新遍历数据,为下一个epoch yield 数据\n",
    "    i = 0\n",
    "    while 1:\n",
    "        X = []\n",
    "        y = []\n",
    "        global x_test, y_test,test_size, batch_size\n",
    "        for cosmology in cosmologies[batch_size * i: batch_size * (i + 1)]:\n",
    "            try:\n",
    "                gridfile = gridfile_dict[cosmology]\n",
    "                griddata = load_grid(gridfile, 'c')\n",
    "                for subcube in subcubes(griddata):\n",
    "                    X.append(subcube)\n",
    "                    y.append(np.array([infos[cosmology]['om'], infos[cosmology]['sigma8']]))\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "        X = np.array(X\n",
    "        y = np.array(y)\n",
    "        x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size)\n",
    "        x_train = x_train.reshape(-1, 32, 32, 32, 1)\n",
    "        x_test = x_test.reshape(-1, 32, 32, 32, 1)\n",
    "        i += 1\n",
    "        yield x_train, y_train  # tuple 类型\n",
    "        # 15个批次后重新遍历数据,此循环即死循环\n",
    "        if i == 465//batch_size:\n",
    "            i = 0\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "from keras import Sequential, layers\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "\n",
    "def create_validate_sample(nsample, use_random=True, startid=None ):\n",
    "    cosmologies = list(gridfile_dict); ncosmo = len(cosmologies)\n",
    "    if use_random:\n",
    "        rows = [np.random.randint(0,ncosmo) for row in range(nsample)];\n",
    "        rows = list(set(rows))\n",
    "        while len(rows) < nsample:\n",
    "            rows = rows + [np.random.randint(0,ncosmo) for row in range(nsample - len(rows))];\n",
    "            rows = list(set(rows))\n",
    "    else:\n",
    "        rows = range(startid, startid+nsample)\n",
    "    x, y =[], []\n",
    "    for row in rows:\n",
    "        cosmology = cosmologies[row]\n",
    "        gridfile = gridfile_dict[cosmology]\n",
    "        griddata = load_grid(gridfile)\n",
    "        for subcube in subcubes(griddata):\n",
    "            x.append(subcube)\n",
    "            y.append(np.array([infos[cosmology]['om'], infos[cosmology]['sigma8']]))\n",
    "    x = np.array(x); x = x.reshape(-1, 32, 32, 32, 1); y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "def plot_test(model, x, y, plot_avg_predict = True, fig=None, ax = None, plot_subpoints=False):\n",
    "    y_predict = model.predict(x); \n",
    "    \n",
    "    if fig == None or ax ==None:\n",
    "        fig, ax = plt.subplots(figsize=(14,6))\n",
    "    cs = range(len(y)); cs = cs / mean(cs)\n",
    "    ax.scatter(y[:,0], y[:,1], c='b', marker='*', label='input', s=200)\n",
    "    if plot_subpoints:\n",
    "        ax.scatter(y_predict[:,0], y_predict[:,1], c='g',  marker='p', s=50, label='outputs')\n",
    "    \n",
    "        \n",
    "    om_test, w_test = y[:,0], y[:,1]\n",
    "    om_predict, w_predict = y_predict[:,0], y_predict[:,1]\n",
    "    \n",
    "    if plot_avg_predict:\n",
    "        ax.scatter(mean(y_predict[:,0]), mean(y_predict[:,1]), marker='*', c='r', label='output_avg', s=200)\n",
    "        ax.plot([om_test[0], mean(y_predict[:,0])], [w_test[0], mean(y_predict[:,1])], lw=2, c='k', ls='--' )\n",
    "\n",
    "    \n",
    "    #for row in range(len(om_predict)):\n",
    "    #    ax.plot( [om_predict[row], om_test[row]], [w_predict[row], w_test[row]], lw=0.5, c='gray' )\n",
    "    ax.set_xlabel(r'$\\Omega_m$',fontsize=16); ax.set_ylabel(r'$\\sigma_8$',fontsize=16)\n",
    "    ax.legend()   \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cosmo/Software/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cosmo/Software/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 300\n",
    "\n",
    "nowmodel = keras.Sequential([\n",
    "        layers.BatchNormalization( input_shape=(32, 32, 32, 1)),\n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "        layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "        layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
    "        layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(2, ),\n",
    "    ])\n",
    "    # momentum, adam, sgd, mini-batch. 并非是局部最小点,极大可能是鞍点,高维空间中鞍点数目远大于最优点,但鞍点的数量在整个空间内是微不足道的\n",
    "    # 真正可能遇到的问题是大面积平坦区域,但是其情况是loss值很高.未知的地形将导致假收敛\n",
    "    # 控制Learning rate为一较小的量\n",
    "    # adam = Adam(lr=1e-4), model.compile(optimizer=adam)\n",
    "    # rmsprop = RMSprop(lr=0.0005, rho=0.9, epsilon=1e-8, decay=0.0); model.compile(optimizer=rmsprop)\n",
    "nowmodel.compile(optimizer=keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8), loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32, 1)     4         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 30, 30, 30, 32)    896       \n",
      "_________________________________________________________________\n",
      "average_pooling3d_1 (Average (None, 15, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 15, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 13, 13, 13, 64)    55360     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_2 (Average (None, 6, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 6, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 4, 4, 4, 128)      221312    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_3 (Average (None, 2, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,590,470\n",
      "Trainable params: 1,590,276\n",
      "Non-trainable params: 194\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nowmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "Begin training for src/model1.1.1_pan/, current epochs =  0 ; max_epochs =  300 ...\n",
      "###############################################\n",
      "Epoch 1/10\n",
      "26/77 [=========>....................] - ETA: 44s - loss: 0.0428 - mean_squared_error: 0.0428"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dada61fec6b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     validation_data=(x_test,y_test))\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 0 \n",
    "\n",
    "step_epoch = 10\n",
    "\n",
    "while epochs <= max_epochs:\n",
    "\n",
    "    print('###############################################')\n",
    "    print('Begin training for '+modelname+', current epochs = ', epochs, '; max_epochs = ', max_epochs, '...')\n",
    "    print('###############################################')\n",
    "    outputf.write('###############################################\\n')\n",
    "    outputf.write('Begin training for '+modelname+', current epochs = '+ str(epochs)+ '; max_epochs = '+str( max_epochs)+ '...\\n')\n",
    "    outputf.write('###############################################\\n')\n",
    "\n",
    "    if True:\n",
    "        nowmodel.fit_generator(train_generator(),\n",
    "                    steps_per_epoch=465//batch_size,  # 数据规格可能大小不对应\n",
    "                    epochs=step_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test))\n",
    "    epochs += step_epoch\n",
    "\n",
    "    filepath = './'+modelname+'/'+str(epochs)+'.save'\n",
    "    # Plot validation\n",
    "    if True:\n",
    "        fig, ax = None, None\n",
    "        for row in range(10):\n",
    "            x_test, y_test = create_validate_sample(1, use_random=True, startid=row)\n",
    "            fig, ax = plot_test(nowmodel, x_test, y_test, fig=fig, ax=ax)\n",
    "        ax.grid(); plt.show()\n",
    "        ax.set_title('#-epochs = '+str(epochs), fontsize=16)\n",
    "        fig.savefig(filepath+'.png', format='png')\n",
    "    print('save model to :', filepath )\n",
    "    outputf.write('save model to :'+str(filepath)+ '\\n')\n",
    "    keras.models.save_model(nowmodel, filepath)\n",
    "\n",
    "outputf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
