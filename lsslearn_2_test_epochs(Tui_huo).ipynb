{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "modelname = 'src/model1.1.1Llocal'\n",
    "\n",
    "os.system('mkdir ./'+modelname)\n",
    "outputf = open(modelname+'/output.txt', 'w')\n",
    "\n",
    "outputf.write('OMP_NUM_THREADS = '+str( os.popen('echo $OMP_NUM_THREADS').read()) +'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import keras, os, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total  465 cosmologies\n",
      "Build up gridfile_dict... (for speed-up of load_grid()) \n",
      "\tmissing cosmology! om0.180_As2.040\n",
      "\tmissing cosmology! om0.180_As2.100\n",
      "\tmissing cosmology! om0.180_As2.120\n",
      "\tmissing cosmology! om0.180_As2.140\n",
      "\tmissing cosmology! om0.180_As2.200\n",
      "\tmissing cosmology! om0.180_As2.220\n",
      "\tmissing cosmology! om0.180_As2.260\n"
     ]
    }
   ],
   "source": [
    "#lsstr = \"ls /home/xiaodongli/data/colas/cola_multiverse/om_As/\"\n",
    "lsstr = \"ls /media/xiaodongli/0B9ADFB4341AD2BD/om_As/\"\n",
    "\n",
    "def cosmostr(om, As):\n",
    "    return 'om%.3f' % om + '_As%.3f' % As\n",
    "\n",
    "def snpfiles(cosmology, snpstr='c'):  #\n",
    "    return os.popen(lsstr + cosmology + \"*snap*\" + snpstr + \".*\").read().split()\n",
    "\n",
    "def gridfiles(cosmology, snpstr='c'):\n",
    "    return os.popen(lsstr + cosmology + \"*grid*\" + snpstr + \".*\").read().split()\n",
    "\n",
    "def mocklist():\n",
    "    files = os.popen(lsstr + \"om*.lua\").read().split('\\n')\n",
    "    # *代替多个字母,即列出所有符合条件的.lua文件:om...\n",
    "    cosmologies = []  # 宇宙学参数\n",
    "    mocks = {}  # 模拟测试\n",
    "    ifile = 0  # 有效文件\n",
    "    for nowfile in files:\n",
    "        # str[a:b]不存在时,返回'',不存在则忽略\n",
    "        nowstr = nowfile[-39:-10]\n",
    "        if nowstr == '':\n",
    "            continue\n",
    "        cosmologies.append(nowstr[0:15])\n",
    "        ifile += 1\n",
    "        try:\n",
    "            mocks[nowstr[0:15]] = {'om': float(nowstr[2:7]), 'As': float(nowstr[10:15]),\n",
    "                                   'sigma8': float(nowstr[23:29])}\n",
    "            # 添加随机数种子\n",
    "            ranseed = float(open(nowfile, 'r').readline().split()[2])  # 默认以所有空字符为分隔符,包括空格,\\n,\\t\n",
    "            mocks[nowstr[0:15]]['ranseed'] = int(ranseed)\n",
    "            # print(ranseed)\n",
    "        except:\n",
    "            pass\n",
    "    return cosmologies, files, mocks\n",
    "\n",
    "gridfile_dict = {}\n",
    "\n",
    "cosmologies, filenames, infos = mocklist()\n",
    "print('In total ', len(cosmologies), 'cosmologies')\n",
    "outputf.write('In total '+str(len(cosmologies))+'cosmologies\\n')\n",
    "\n",
    "print('Build up gridfile_dict... (for speed-up of load_grid()) ')\n",
    "outputf.write('Build up gridfile_dict... (for speed-up of load_grid()) \\n')\n",
    "for cosmology in cosmologies:\n",
    "    rlt = gridfiles(cosmology)\n",
    "    if rlt == []:\n",
    "        print ('\\tmissing cosmology!', cosmology)\n",
    "        outputf.write('\\tmissing cosmology!' +str(cosmology)+ '\\n')\n",
    "    else:\n",
    "        gridfile_dict[cosmology] = rlt[0]\n",
    "np.random.shuffle(cosmologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid(gridfile, snpstr='c', printinfo=False):  # 网格加载\n",
    "    #gridfile = os.popen(lsstr + cosmology+\"_sigma8_*grid*\" + snpstr + \".*\").read().split()[0]\n",
    "    #print('load in gridfile : ', gridfile, '...')\n",
    "    nowf = open(gridfile, 'rb')  # 以二进制形式读取文件\n",
    "    # struct:对python基本类型值与用python字符串格式表示的C struct类型间转化\n",
    "    size = struct.unpack('f' * 1, nowf.read(4 * 1))[0]\n",
    "    grid_nc = struct.unpack('i' * 1, nowf.read(4 * 1))[0]\n",
    "    data = struct.unpack('f' * grid_nc ** 3, nowf.read(4 * grid_nc ** 3))\n",
    "    if printinfo:\n",
    "        print('read in box size     \\n\\t', size)\n",
    "        print('read in num_grid      \\n\\t', grid_nc)\n",
    "        print('read in coarse grid \\n\\tsize    : ', len(data), '\\n\\texpect  : ', grid_nc ** 3)\n",
    "\n",
    "    nowf.close()\n",
    "    return np.array(data).reshape((grid_nc, grid_nc, grid_nc))\n",
    "\n",
    "def subcubes(A):\n",
    "    rlt = []\n",
    "    for row1 in [0, 32, 64, 96]:\n",
    "        for row2 in [0, 32, 64, 96]:\n",
    "            for row3 in [0, 32, 64, 96]:\n",
    "                rlt.append(A[row1:row1+32,row2:row2+32,row3:row3+32])\n",
    "    return rlt\n",
    "\n",
    "def data_augument(A):\n",
    "    rlt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "batch_size = 31\n",
    "num_subcube = 64\n",
    "num_data_augument = 48\n",
    "\n",
    "x_train = np.zeros((batch_size-int(test_size*batch_size)-1, 2))  # 因test_size=0.3,保证初始化矩阵的形状与后面的相同\n",
    "x_test = np.zeros((int(test_size*batch_size)+1,32,32,32, 1))\n",
    "y_test = np.zeros((int(test_size*batch_size)+1, 2))\n",
    "y_train = np.zeros((batch_size-int(test_size*batch_size)-1, 2))\n",
    "\n",
    "\n",
    "###  xiaodong: 重新写了 load_grid 程序。。。之前有错误！！！！（好像只会 load 进来一个 om...)\n",
    "def train_generator():  # 必须无限循环yield数据,全部数据遍历后再重新遍历数据,为下一个epoch yield 数据\n",
    "    i = 0\n",
    "    while 1:\n",
    "        X = []\n",
    "        y = []\n",
    "        global x_train, x_test, y_test, y_train, test_size, batch_size\n",
    "        #print(' load in ', batch_size * i, 'to', batch_size * (i + 1), '... len(cosmologies)=',\n",
    "        #      len(cosmologies))\n",
    "        for cosmology in cosmologies[batch_size * i: batch_size * (i + 1)]:\n",
    "            try:\n",
    "                gridfile = gridfile_dict[cosmology]\n",
    "                gridfile_exist = True\n",
    "            except:\n",
    "                #print('skip cosmology ', cosmology, '!!!')\n",
    "                gridfile_exist = False\n",
    "            if gridfile_exist:\n",
    "                griddata = load_grid(gridfile, 'c')\n",
    "                for subcube in subcubes(griddata):\n",
    "                    X.append(subcube)\n",
    "                    y.append(np.array([infos[cosmology]['om'], infos[cosmology]['sigma8']]))\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size)\n",
    "        x_train = x_train.reshape(-1, 32, 32, 32, 1)\n",
    "        x_test = x_test.reshape(-1, 32, 32, 32, 1)\n",
    "        i += 1\n",
    "        yield x_train, y_train  # tuple 类型\n",
    "        # 15个批次后重新遍历数据,此循环即死循环\n",
    "        if i == 465//batch_size:\n",
    "            i = 0\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "from keras import Sequential, layers\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "\n",
    "def create_validate_sample(nsample, use_random=True, startid=None ):\n",
    "    cosmologies = list(gridfile_dict); ncosmo = len(cosmologies)\n",
    "    if use_random:\n",
    "        rows = [np.random.randint(0,ncosmo) for row in range(nsample)];\n",
    "        rows = list(set(rows))\n",
    "        while len(rows) < nsample:\n",
    "            rows = rows + [np.random.randint(0,ncosmo) for row in range(nsample - len(rows))];\n",
    "            rows = list(set(rows))\n",
    "    else:\n",
    "        rows = range(startid, startid+nsample)\n",
    "    x, y =[], []\n",
    "    for row in rows:\n",
    "        cosmology = cosmologies[row]\n",
    "        gridfile = gridfile_dict[cosmology]\n",
    "        griddata = load_grid(gridfile)\n",
    "        for subcube in subcubes(griddata):\n",
    "            x.append(subcube)\n",
    "            y.append(np.array([infos[cosmology]['om'], infos[cosmology]['sigma8']]))\n",
    "    x = np.array(x); x = x.reshape(-1, 32, 32, 32, 1); y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "def plot_test(model, x, y, plot_avg_predict = True, fig=None, ax = None, plot_subpoints=False):\n",
    "    y_predict = model.predict(x); \n",
    "    \n",
    "    if fig == None or ax ==None:\n",
    "        fig, ax = plt.subplots(figsize=(14,6))\n",
    "    cs = range(len(y)); cs = cs / mean(cs)\n",
    "    ax.scatter(y[:,0], y[:,1], c='b', marker='*', label='input', s=200)\n",
    "    if plot_subpoints:\n",
    "        ax.scatter(y_predict[:,0], y_predict[:,1], c='g',  marker='p', s=50, label='outputs')\n",
    "    \n",
    "        \n",
    "    om_test, w_test = y[:,0], y[:,1]\n",
    "    om_predict, w_predict = y_predict[:,0], y_predict[:,1]\n",
    "    \n",
    "    if plot_avg_predict:\n",
    "        ax.scatter(mean(y_predict[:,0]), mean(y_predict[:,1]), marker='*', c='r', label='output_avg', s=200)\n",
    "        ax.plot([om_test[0], mean(y_predict[:,0])], [w_test[0], mean(y_predict[:,1])], lw=2, c='k', ls='--' )\n",
    "\n",
    "    \n",
    "    #for row in range(len(om_predict)):\n",
    "    #    ax.plot( [om_predict[row], om_test[row]], [w_predict[row], w_test[row]], lw=0.5, c='gray' )\n",
    "    ax.set_xlabel(r'$\\Omega_m$',fontsize=16); ax.set_ylabel(r'$\\sigma_8$',fontsize=16)\n",
    "    ax.legend()   \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 500\n",
    "\n",
    "nowmodel = keras.Sequential([\n",
    "        layers.BatchNormalization( input_shape=(32, 32, 32, 1)),\n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "        layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "        layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
    "        layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(2, ),\n",
    "    ])\n",
    "    # momentum, adam, sgd, mini-batch. 并非是局部最小点,极大可能是鞍点,高维空间中鞍点数目远大于最优点,但鞍点的数量在整个空间内是微不足道的\n",
    "    # 真正可能遇到的问题是大面积平坦区域,但是其情况是loss值很高.未知的地形将导致假收敛\n",
    "    # 控制Learning rate为一较小的量\n",
    "    # adam = Adam(lr=1e-4), model.compile(optimizer=adam)\n",
    "    # rmsprop = RMSprop(lr=0.0005, rho=0.9, epsilon=1e-8, decay=0.0); model.compile(optimizer=rmsprop)\n",
    "nowmodel.compile(optimizer=keras.optimizers.Adadelta(), loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, 32, 32, 32, 1)     4         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 30, 30, 30, 32)    896       \n",
      "_________________________________________________________________\n",
      "average_pooling3d_10 (Averag (None, 15, 15, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 15, 15, 15, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 13, 13, 13, 64)    55360     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_11 (Averag (None, 6, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 6, 6, 6, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 4, 4, 4, 128)      221312    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_12 (Averag (None, 2, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,590,470\n",
      "Trainable params: 1,590,276\n",
      "Non-trainable params: 194\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nowmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "Begin training for src/model1.1.1Llocal, current epochs =  0 ; max_epochs =  500 ...\n",
      "###############################################\n",
      "Epoch 1/10\n",
      "12/15 [=======================>......] - ETA: 1:45 - loss: 0.0104 - mean_squared_error: 0.0104"
     ]
    }
   ],
   "source": [
    "epochs = 0 \n",
    "\n",
    "step_epoch = 10\n",
    "\n",
    "while epochs <= max_epochs:\n",
    "\n",
    "    print('###############################################')\n",
    "    print('Begin training for '+modelname+', current epochs = ', epochs, '; max_epochs = ', max_epochs, '...')\n",
    "    print('###############################################')\n",
    "    outputf.write('###############################################\\n')\n",
    "    outputf.write('Begin training for '+modelname+', current epochs = '+ str(epochs)+ '; max_epochs = '+str( max_epochs)+ '...\\n')\n",
    "    outputf.write('###############################################\\n')\n",
    "\n",
    "    if True:\n",
    "        nowmodel.fit_generator(train_generator(),\n",
    "                    steps_per_epoch=465//batch_size,  # 数据规格可能大小不对应\n",
    "                    epochs=step_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test))\n",
    "    epochs += step_epoch\n",
    "\n",
    "    filepath = './'+modelname+'/'+str(epochs)+'.save'\n",
    "    # Plot validation\n",
    "    if True:\n",
    "        fig, ax = None, None\n",
    "        for row in range(10):\n",
    "            x_test, y_test = create_validate_sample(1, use_random=True, startid=row)\n",
    "            fig, ax = plot_test(nowmodel, x_test, y_test, fig=fig, ax=ax)\n",
    "        ax.grid(); plt.show()\n",
    "        ax.set_title('#-epochs = '+str(epochs), fontsize=16)\n",
    "        fig.savefig(filepath+'.png', format='png')\n",
    "    print('save model to :', filepath )\n",
    "    outputf.write('save model to :'+str(filepath)+ '\\n')\n",
    "    keras.models.save_model(nowmodel, filepath)\n",
    "\n",
    "outputf.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
